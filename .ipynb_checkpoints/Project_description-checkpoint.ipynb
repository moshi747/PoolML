{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191e612-8327-43ad-b448-a495d67eba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Rough outline:\n",
    "\n",
    "Introduction:\n",
    "    Background on pool\n",
    "    Goal of this project\n",
    "\n",
    "Vision\n",
    "    Showcase using a few pictures\n",
    "    Rough description on how the transform is done\n",
    "    \n",
    "Algorithm\n",
    "    Domain knowledge and feature engineering\n",
    "    Generating data\n",
    "    Models\n",
    "    Sample shots (show using the different models)\n",
    "    Performance analysis (how they do on average against random shots)\n",
    "        Baseline is random shot selection + constant shot selection\n",
    "\n",
    "Conclusion and future plans\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70e6b2-9e40-4fba-b5b3-c0f74d893269",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Background on pool\n",
    "\n",
    "In pool, a general goal is to use the cue ball (white ball) to pot an object ball (numbered ball) into a pocket. In many games, a critical part is to not only pot the object ball but to also leave the cue ball in a good position for the next shot (see https://en.wikipedia.org/wiki/Nine-ball). This is called *playing position* and can be achieved by varying the three scalars below:\n",
    "\n",
    "speed, horizontal strike position (x-strike), vertical strike position (y-strike)\n",
    "\n",
    "# Goal of this project\n",
    "\n",
    "The goal is to use machine learning to approximate the following function\n",
    "\n",
    "$$f(\\text{position of the balls (cue ball + object balls)})= \\text{speed, x-strike, y-strike}$$\n",
    "\n",
    "which outputs the best way to hit the cue ball to play position given the current layout of the balls. As most experienced pool players know, pool is a game of milimeters meaning the slightest of change is the layout of the balls can have a big impact on how you would play. Hence the ideal function $f$ would be very complex, expecially if there are many object balls.\n",
    "\n",
    "We will rely on an open source pool simulator (https://ekiefl.github.io/projects/pooltool/) developed by Evan Kiefl.\n",
    "\n",
    "There will be two parts. The first part is to use computer vision to analyze a picture of a pool table taken by the user. The algorithm will then detect the playing surface of the table along with the location of the balls. The pool balls will be detected using a fine-tuned model of YOLO-V8. The second part is creating a ML model that learns the function $f$.\n",
    "\n",
    "# Detecting ball locations from picture\n",
    "\n",
    "Our first task is to determine the playing surface of the table. Since pool cloth usually have two common colors (green or blue), we use simple masking to find a large area where the color is constant. We will use the following image for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac633f4-2b0a-415f-a8cf-0a4881bd2f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36f121f7-f674-4b2b-bdb4-f6926ebb1cd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating a ML model for $f$ \n",
    "\n",
    "\n",
    "### Simplest case\n",
    "\n",
    "For now, we will consider the simplest case of when there are two object balls. In order for the AI to learn how effective a particular shot was, we need a way to value the result (the location of the cue ball and the remaining object ball after potting). While this can probably be done by some geometric calculation, we will use a DNN to achieve this.\n",
    "\n",
    "### Creating a DNN for valuating layouts\n",
    "\n",
    "We use supervised learning. Our data will consist of the position of the cue ball and object ball together with an integer $0\\leq n\\leq 100$ which measures the difficulty of the shot ($0$ being most difficult and $100$ being easiest).\n",
    "\n",
    "So there are $4$ feature cue-x, cue-y, one-x, one-y for the $xy$-coordinates of the balls and one output $n$. To construct the data, we generate the positions of the balls randomly and simulate $100$ shots where we slightly adjust the aiming point every time. The number of times we pot the object ball will be our $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b605d-9cf6-43ad-964e-dc04f0fa556e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data import generate_data\n",
    "\n",
    "one_ball_data = generate_data(10000, tests=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adfd3d9-2896-45b3-b51a-b619e99168c3",
   "metadata": {},
   "source": [
    "Once enough data has been generated, we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6386e4-d753-46d1-81be-5e5d3b99be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regression import train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_ball_data.iloc[:,:-1], one_ball_data.iloc[:,-1:])\n",
    "\n",
    "reg_net, train_losses, test_losses = train(X_train, X_test, y_train, y_test,\n",
    "                                           hidden_sizes = [128, 128, 128], epochs = 10000, lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d6872-116c-4afe-87c1-53f679376113",
   "metadata": {},
   "source": [
    "Let's visualize the CV scores to see how the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b6032-06e1-4d4f-b2d8-bf07ecaef3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add plot here\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.plot(np.arange(len(train_losses)), train_losses, label='train loss')\n",
    "\n",
    "plt.plot(np.arange(len(test_losses)), test_losses, label='test loss')\n",
    "\n",
    "ax = fig.axes[0]\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8922917-566f-4cd1-89eb-410115778581",
   "metadata": {},
   "source": [
    "Now that we have a baseline model for valuating the resulting position, let's move on to the next step of our original goal.\n",
    "\n",
    "### Two methods\n",
    "\n",
    "We present two different methods for constructing a model\n",
    "\n",
    "* Supervised learning\n",
    "\n",
    "* Reinforcement learning\n",
    "\n",
    "In both cases, the goal is to learn the following function\n",
    "\n",
    "$$Q (\\text{cue-x, cue-y, one-x, one-y, two-x, two-y, speed, x-spin, y-spin}) = n\\in [0,100]$$\n",
    "\n",
    "which is defined as valuating the resulting position from shooting the cue ball in the given way potting the one ball. In reinforcement learning, the algorithm will also output a policy function which takes input the positions of the balls and outputs the hit on the cue ball that results in the highest output of the $Q$-function.\n",
    "\n",
    "Let us now go into the details for each one\n",
    "\n",
    "### Supervised learning model\n",
    "\n",
    "In this case, our goal again is to construct a bunch of data by randomly generating points in the domain of $Q$ and logging its output. This is done using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484feadd-7d49-4bbf-9484-714f1850d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import generate_position_data\n",
    "\n",
    "two_ball_data = generate_position_data(2000, reg_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d7132-d489-4618-9876-6ff4ae1b23cb",
   "metadata": {},
   "source": [
    "We then use the same code as before to train a DNN on this data. The performance is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753f7d8-c8cf-4a00-89fd-99868cbccfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate performance here\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(two_ball_data.iloc[:,:-1], two_ball_data.iloc[:,-1:])\n",
    "\n",
    "reg_net_2, train_losses, test_losses = train(X_train, X_test, y_train, y_test,\n",
    "                                             hidden_sizes = [128, 128, 128, 128], epochs = 10000, lr = 0.001)\n",
    "\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.plot(np.arange(len(train_losses)), train_losses, label='train loss')\n",
    "\n",
    "plt.plot(np.arange(len(test_losses)), test_losses, label='test loss')\n",
    "\n",
    "ax = fig.axes[0]\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69370a87-3d42-4769-a45b-69f6283030ac",
   "metadata": {},
   "source": [
    "The MSE here is higher than the case with a single object ball. The function we're trying to learn here is much more intricate. We also tried XGBoost, but the results were not as goot as the neural network.\n",
    "\n",
    "### Reinforcement learning model\n",
    "\n",
    "Here we created a GYM API for this task. The natural choices for the states are the position of the balls and the transition actions are given by the hit on the cue ball. The interesting part is setting up the reward function for the actions. We settled on the following\n",
    "\n",
    "1. If object ball is not potted:\n",
    "\n",
    "    a. If no object ball moves: $-10$\n",
    "    \n",
    "    b. Else: $-0.1$\n",
    "    \n",
    "2. Elif cue ball scratches (goes into a pocket): $-4$\n",
    "\n",
    "3. Elif both object balls are potted: $0$\n",
    "\n",
    "4. Else (target object ball potted and two balls remain on the table): $4(e^{n/100}-1)$ where $n$ is the valuation of the resulting position.\n",
    "\n",
    "The reason we chose to include an exponential function in the last case is to emphasize the importance of getting as close to the largest $n$ as possible by offering a reward that is weighted exponentially rather than linealry on $n$. This seem to work well in practice as our experiments showed. The exact algorithm we employ is the soft actor-critic method given here https://spinningup.openai.com/en/latest/algorithms/sac.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b5547-91ef-43b2-b028-efd63b47b47f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spinup.algos.pytorch.sac.sac import sac\n",
    "from env import Pool_random\n",
    "\n",
    "sac(Pool_random, epochs = 10, alpha = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69bfa21-ed1e-474f-bbdc-f64bce6db7c4",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that we have the case of two balls finished, we can move on to 3, 4, ..., and eventually 9 balls for the game of 9-ball. The inductive step is as follows: Once we have a model for $n$ many balls, we can use our model to construct an auxilary model which learns the *value function* attached to any layout of $n$ balls. This value function can be thought of as the maximum possible reward attainable by following the optimal shooting choices (which we have a model for). Then we may construct a model for $n+1$ balls by either supervised or reinforcement learning as before trying to maximize the value function of the remaining layout after potting.\n",
    "\n",
    "Another approach would be to simply apply a reinforcement or supervised learning algorithm to the case of 9 balls. In practice, we found the complexity coming from the number of balls makes it very hard to optimize the model. Hence, we are taking an iterative approach as shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d51574-f1b6-40ac-be3b-21607d077472",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "While a random layout of 9 balls seemed too much for reinforcement learning to optimize, having a fixed starting layout is much easier. Here we demonstrate using the GYM environemnt Pool_static which takes as input a fixed starting layout of balls and always starts here whenever reset is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c7afb-a520-485a-9fba-6c75a6f8a125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from env import Pool_static\n",
    "\n",
    "sac(Pool_random, epochs = 10, alpha = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d2f029-bc0a-4e58-913d-dce6465e7798",
   "metadata": {},
   "source": [
    "Here is a video demonstrating the shots learned for a particular layout of 9-ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9c05a-2b43-4bbe-8347-1dc744433bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
